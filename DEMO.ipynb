{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Standalone Demo of an optimized Random Forest Fork Classifier\n",
    "Machine Learning For Robotics (RO47002) - Final Assignment \n",
    "\n",
    "Tom Kerssemakers - 4345487, Oyono Ramos Lourenço de Armada - 4459059\n",
    "\n",
    "\n",
    "#### Scroll to the last cell for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sklearn as sk\n",
    "import scipy\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.util\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import ipywidgets\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib.patches import Circle\n",
    "from IPython.display import display\n",
    "from collections import OrderedDict\n",
    "from scipy import ndimage as ndi\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from scipy.spatial import distance\n",
    "from skimage import feature, data, io, filters\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "\n",
    "def eval_classifier(clf, X, y):\n",
    "    y_pred=clf.predict(X)\n",
    "    accuracy=accuracy_score(y,y_pred)\n",
    "    confmat=confusion_matrix(y,y_pred)\n",
    "    return y_pred, accuracy, confmat\n",
    "\n",
    "def report_eval(name, accuracy, confmat):\n",
    "    print(f'*** {name} ***')\n",
    "    print(f' confusion matrix:')\n",
    "    print(confmat)\n",
    "    print(f' accuracy: {accuracy:.3f}')\n",
    "\n",
    "def load_and_resize_image(filename, overwrite=False):\n",
    "    MAX_SIZE = 512\n",
    "    I = plt.imread(filename)\n",
    "    \n",
    "    # is the width or height too large?\n",
    "    oversize_ratio = max(I.shape[0] / MAX_SIZE, I.shape[1] / MAX_SIZE)\n",
    "    if oversize_ratio > 1.0:\n",
    "        print('resizing image ...')\n",
    "        I = skimage.transform.rescale(I, (1./oversize_ratio, 1/oversize_ratio, 1), anti_aliasing=True)\n",
    "        \n",
    "        # overwrite the old image file after resizing\n",
    "        if overwrite:\n",
    "            print(f'overwriting \"{filename}\" ...')\n",
    "            plt.imsave(filename, I)\n",
    "            \n",
    "    return I\n",
    "\n",
    "\n",
    "def BW_filter(filenames):\n",
    "    Ibw = [skimage.io.imread(filename, as_gray=True) for filename in filenames]\n",
    "    Ibwf = np.empty([len(Ibw),512,512])\n",
    "    for i in range(len(Ibw)):\n",
    "        Ibwf[i,:,:] = filters.sobel(Ibw[i])\n",
    "    print('...all %i images have been transformed to B&W and filtered...' %len(Ibwf))\n",
    "    return Ibwf\n",
    "\n",
    "# Get Image information\n",
    "def get_image_width(I):\n",
    "    height, width = I.shape\n",
    "    return(width)\n",
    "\n",
    "def get_image_height(I):\n",
    "    height, width = I.shape\n",
    "    return(height)\n",
    "\n",
    "# Remove points that are too close to the border of the figure\n",
    "def remove_points_near_border(I, points, HALF_WIN_SIZE):\n",
    "    W = get_image_width(I)\n",
    "    H = get_image_height(I)\n",
    "\n",
    "    # discard points that are too close to border\n",
    "    points = points[points[:,0] > HALF_WIN_SIZE[1],:]\n",
    "    points = points[points[:,1] > HALF_WIN_SIZE[0],:]\n",
    "    points = points[points[:,0] < W - HALF_WIN_SIZE[1],:]\n",
    "    points = points[points[:,1] < H - HALF_WIN_SIZE[0],:]\n",
    "    \n",
    "    return points\n",
    "\n",
    "def sample_points_grid(I, WIN_SIZE, HALF_WIN_SIZE):\n",
    "    # window centers\n",
    "    W = get_image_width(I)\n",
    "    H = get_image_height(I)\n",
    "    \n",
    "    step_size = (WIN_SIZE[0]//3, WIN_SIZE[1]//3)\n",
    "    \n",
    "    center_ys = range(HALF_WIN_SIZE[0], H-HALF_WIN_SIZE[0]+1, step_size[0])\n",
    "    center_xs = range(HALF_WIN_SIZE[1], W-HALF_WIN_SIZE[1]+1, step_size[1])\n",
    "    centers = np.array(np.meshgrid(center_xs, center_ys))\n",
    "    centers = centers.reshape(2,-1).T\n",
    "    centers = centers.astype(float) \n",
    "    \n",
    "    # add a bit of random offset\n",
    "    np.random.seed(1)\n",
    "    centers += np.random.rand(*centers.shape) * 10 \n",
    "    \n",
    "    # discard points close to border where we can't extract patches\n",
    "    centers = remove_points_near_border(I, centers, HALF_WIN_SIZE)\n",
    "    \n",
    "    return centers\n",
    "\n",
    "def sample_points_around_targets2(I, p1, p2, WIN_SIZE, HALF_WIN_SIZE):\n",
    "    Nt = 200 # samples at target locations, i.e. near start, end, and middle of cutlery\n",
    "    target_std_dev = np.array(HALF_WIN_SIZE[:2])/1\n",
    "    # sample around target locations\n",
    "    tpoints1 = np.random.randn(Nt,2)\n",
    "    tpoints1 = tpoints1 * target_std_dev + p1\n",
    "    tpoints2 = np.random.randn(Nt,2)\n",
    "    tpoints2 = tpoints2 * target_std_dev + p2\n",
    "    points = np.vstack((tpoints1, tpoints2))\n",
    "    # discard points close to border where we cannot extract patches\n",
    "    points = remove_points_near_border(I, points, HALF_WIN_SIZE)\n",
    "    return points\n",
    "\n",
    "# Collect patch at generated points\n",
    "def get_patch_at_point(I, p, HALF_WIN_SIZE):\n",
    "    p = p.astype(int)\n",
    "    P = np.array(I[p[1]-HALF_WIN_SIZE[1]:p[1]+HALF_WIN_SIZE[1], p[0]-HALF_WIN_SIZE[1]:p[0]+HALF_WIN_SIZE[1]])  \n",
    "    return P\n",
    "\n",
    "# Turn patches into feature vectors\n",
    "\n",
    "def patch_to_vec(P):\n",
    "    FEAT_SIZE = (9,9,1)\n",
    "    tensor = skimage.transform.resize(P, FEAT_SIZE)\n",
    "    x = tensor.flatten()    \n",
    "    return x\n",
    "\n",
    "# Define label for all points\n",
    "def make_labels_for_points(I, p1, p2, points, WIN_SIZE):\n",
    "    \"\"\" Determine the class label (as an integer) on point distance to different parts of the pen \"\"\"\n",
    "    num_points = points.shape[0]\n",
    "    \n",
    "    # for all points ....\n",
    "    # ... determine their distance to tip of the cutlery\n",
    "    dist1 = points - p1\n",
    "    dist1 = np.sqrt(np.sum(dist1 * dist1, axis=1))\n",
    "    \n",
    "    # ... determine their distance to end of the cutlery\n",
    "    dist2 = points - p2\n",
    "    dist2 = np.sqrt(np.sum(dist2 * dist2, axis=1))\n",
    "    \n",
    "     # ... determine distance to cutlery middle\n",
    "    alpha = np.linspace(0.2, 0.8, 100)\n",
    "    midpoints = p1[None,:] * alpha[:,None] + p2[None,:] * (1. - alpha[:,None]) \n",
    "    dist3 = scipy.spatial.distance_matrix(midpoints, points)\n",
    "    dist3 = np.min(dist3, axis=0)\n",
    "    \n",
    "    # the class label of a point will be determined by which distance is smallest\n",
    "    #    and if that distance is at least below `dist_thresh`, otherwise it is background\n",
    "    dist_thresh = WIN_SIZE[0] * 2./3.\n",
    "\n",
    "    # store distance to closest point in each class in columns\n",
    "    class_dist = np.zeros((num_points, 4))\n",
    "    class_dist[:,0] = dist1\n",
    "    class_dist[:,1] = dist2\n",
    "    class_dist[:,2] = dist3\n",
    "    class_dist[:,3] = dist_thresh\n",
    "    \n",
    "    # the class label is now the column with the lowest number\n",
    "    labels = np.argmin(class_dist, axis=1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def extract_patches(I, p1, p2, WIN_SIZE, HALF_WIN_SIZE):\n",
    "        # sample points around target\n",
    "        points = sample_points_around_targets2(I, p1, p2, WIN_SIZE, HALF_WIN_SIZE)\n",
    "        \n",
    "        # determine the labels of the points\n",
    "        labels = make_labels_for_points(I, p1, p2, points, WIN_SIZE)\n",
    "        xs = []\n",
    "        for p in points:\n",
    "            P = get_patch_at_point(I, p, HALF_WIN_SIZE)\n",
    "            x = patch_to_vec(P)\n",
    "            xs.append(x)\n",
    "        X = np.array(xs)\n",
    "        return X, labels, points\n",
    "    \n",
    "def extract_patches2(I, p1, p2, WIN_SIZE, HALF_WIN_SIZE, dist):\n",
    "        # sample points around target\n",
    "        points = sample_points_around_targets2(I, p1, p2, WIN_SIZE, HALF_WIN_SIZE)\n",
    "\n",
    "        # determine the labels of the points\n",
    "        labels = make_labels_for_points(I, p1, p2, points, WIN_SIZE)\n",
    "        xs = []\n",
    "        for p in points:\n",
    "            P = get_patch_at_point(I, p, HALF_WIN_SIZE)\n",
    "            x = patch_to_vec(P)\n",
    "            xs.append(x)\n",
    "        X = np.array(xs)\n",
    "        return X, labels, points\n",
    "\n",
    "def show_pickuppoint(Ibwf, Is, idx, pickuppoints_all, gridpoints_all, points_all, p1_all, p2_all,bestgridpoints1_all,bestgridpoints2_all, best_mean1_all, best_mean2_all, showgridpoints, showpoints, showfilter):\n",
    "    # load image and values\n",
    "    if showfilter == True:\n",
    "        I = Ibwf[idx]\n",
    "    else: \n",
    "        I = Is[idx]\n",
    "    # choose index of current image \n",
    "    pickup = pickuppoints_all[idx]\n",
    "    gridpoints = gridpoints_all[idx]\n",
    "    points = points_all[idx]\n",
    "    p1 = p1_all[idx]\n",
    "    p2 = p2_all[idx]\n",
    "    bestgridpoints1=bestgridpoints1_all[idx]\n",
    "    bestgridpoints2=bestgridpoints2_all[idx]\n",
    "    best_mean1 = best_mean1_all[idx]\n",
    "    best_mean2 = best_mean2_all[idx]\n",
    "    \n",
    "    # Plot figure\n",
    "    plt.figure()\n",
    "    plt.imshow(I, cmap=plt.cm.gray)\n",
    "    \n",
    "    if showpoints == True:\n",
    "        plt.plot(points[:,0], points[:,1], '.y')\n",
    "        plt.plot(best_mean1[0], best_mean1[1], '*c')\n",
    "        plt.plot(best_mean2[0], best_mean2[1], '*c')\n",
    "    \n",
    "    if showgridpoints == True:\n",
    "        plt.plot(gridpoints[:,0], gridpoints[:,1], '.g')\n",
    "        plt.plot(p1[0], p1[1], 'bx')\n",
    "        plt.plot(p2[0], p2[1], 'bx')\n",
    "        plt.plot(bestgridpoints1[:,0],bestgridpoints1[:,1], '<m')\n",
    "        plt.plot(bestgridpoints2[:,0],bestgridpoints2[:,1], '>m')\n",
    "        \n",
    "        \n",
    "    plt.plot(pickup[0],pickup[1],'rx')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "# IMAGE PREDICTOR\n",
    "# Create validation set of full pictures\n",
    "def image_predictior(image_dir, clf, show):\n",
    "\n",
    "    #Import and resize data if needed\n",
    "    filenames = glob.glob(os.path.join(image_dir, '*.jpg')) #     filenames.append(glob.glob(os.path.join(image_dir, '*.jpeg')))\n",
    "\n",
    "    filenames = sorted(filenames)\n",
    "    N = len(filenames)\n",
    "    print(f'found {N} images in target directory')\n",
    "    \n",
    "\n",
    "    overwrite = True\n",
    "    Is = [load_and_resize_image(filename, overwrite) for filename in filenames]\n",
    "    print('loaded %d images' % len(Is))\n",
    "    \n",
    "    # PreProcessing of Images\n",
    "    Ibwf = BW_filter(filenames)\n",
    "    \n",
    "    \n",
    "    # Initialize storage\n",
    "    WIN_SIZE = (50, 50, 1)\n",
    "    HALF_WIN_SIZE = (WIN_SIZE[0] // 2, WIN_SIZE[1] // 2, WIN_SIZE[2])\n",
    "    y_pred_all = []\n",
    "    points_all = []\n",
    "    imgids_all = []\n",
    "    labels_all = []\n",
    "    pickuppoint_all = []\n",
    "    gridpoints_all = []\n",
    "    p1_all = []\n",
    "    p2_all = []\n",
    "    bestgridpoints1_all = []\n",
    "    bestgridpoints2_all = []\n",
    "    best_mean1_all = []\n",
    "    best_mean2_all = []\n",
    "\n",
    "    \n",
    "    # For every image:\n",
    "    for idx in range(len(Ibwf)):\n",
    "        I = Ibwf[idx]\n",
    "        # Get uniform search point grid\n",
    "        gridpoints = sample_points_grid(I, WIN_SIZE, HALF_WIN_SIZE)\n",
    "        gridpoints = remove_points_near_border(I, gridpoints, HALF_WIN_SIZE)\n",
    "        \n",
    "        # Get patches from points and store in one big feature vector\n",
    "        xs=[]\n",
    "        for p in gridpoints:\n",
    "            patch = get_patch_at_point(I, p, HALF_WIN_SIZE)\n",
    "            x = patch_to_vec(patch)\n",
    "            xs.append(x)\n",
    "        xs=np.vstack(xs)\n",
    "          \n",
    "        # Predict probability of the grid patches to find tip and end of the cutlery\n",
    "        clas = clf.predict(xs)\n",
    "        prob = clf.predict_proba(xs)\n",
    "        # Use points with high probability of a class\n",
    "        best_idx0s = np.where(prob[:,0]>=0.90*max(prob[:,0]))\n",
    "        best_idx1s = np.where(prob[:,1]>=0.90*max(prob[:,1]))\n",
    "        bestgridpoints1 = gridpoints[best_idx0s]\n",
    "        bestgridpoints2 = gridpoints[best_idx1s]\n",
    "        p1 = np.mean(bestgridpoints1,axis=0,dtype=np.int)\n",
    "        p2 = np.mean(bestgridpoints2,axis=0,dtype=np.int)\n",
    "        dist = round(distance.euclidean(p1,p2))\n",
    "        \n",
    "        # Create new points around these points\n",
    "        Xs, labels, points = extract_patches2(I, p1, p2, WIN_SIZE, HALF_WIN_SIZE, dist)\n",
    "             \n",
    "        # Predict classes and probabilities\n",
    "        y_pred = clf.predict(Xs)\n",
    "        y_pred_prob = clf.predict_proba(Xs)\n",
    "\n",
    "        best_range1 = np.where(y_pred_prob[:,0]>=0.9*max(y_pred_prob[:,0]))\n",
    "        best_range2 = np.where(y_pred_prob[:,1]>=0.9*max(y_pred_prob[:,1]))\n",
    "        bestpoints1=np.array(points[best_range1])\n",
    "        bestpoints2=points[best_range2]\n",
    "        best_mean1=np.median(bestpoints1,axis=0)\n",
    "        best_mean2=np.median(bestpoints2,axis=0)\n",
    "        pickpoint=list([(best_mean1[0]+best_mean2[0])/2, (best_mean1[1]+best_mean2[1])/2])\n",
    "        \n",
    "        # Save values in order to plot\n",
    "        gridpoints_all.append(gridpoints)\n",
    "        p1_all.append(p1)\n",
    "        p2_all.append(p2)\n",
    "        points_all.append(points)\n",
    "        y_pred_all.append(y_pred)\n",
    "        labels_all.append(labels)\n",
    "        imgids_all.append(np.ones(len(labels),dtype=int)*idx)\n",
    "        pickuppoint_all.append(pickpoint)\n",
    "        bestgridpoints1_all.append(bestgridpoints1)\n",
    "        bestgridpoints2_all.append(bestgridpoints2)\n",
    "        best_mean1_all.append(best_mean1)\n",
    "        best_mean2_all.append(best_mean2)\n",
    "        \n",
    "    pickuppoints_all = np.vstack(pickuppoint_all)\n",
    "    \n",
    "    # Plot results\n",
    "    if show == True:\n",
    "        \n",
    "        \n",
    "        def plot_nth_pickuppoint(n, showgridpoints, showpoints, showfilter):\n",
    "            show_pickuppoint(Ibwf,Is, n, pickuppoints_all, gridpoints_all, points_all, p1_all, p2_all, bestgridpoints1_all, bestgridpoints2_all, best_mean1_all, best_mean2_all, showgridpoints, showpoints, showfilter)\n",
    "            \n",
    "        ipywidgets.interact(plot_nth_pickuppoint,n=(0,len(Ibwf)-1), showgridpoints=(0,1), showpoints=(0,1), showfilter=(0,1))\n",
    "    \n",
    "    return pickuppoint_all, Ibwf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "In this demo the resulting classifier, created in the Final Assignment is presented. First the results on the testdata is shown and in the end a ipywidget is introduced in order to do some validation on un-annoted data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Testdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Random Forest New - TEST ***\n",
      " confusion matrix:\n",
      "[[2165    0   49    0]\n",
      " [   1 1836  247    6]\n",
      " [   2   24 1619   11]\n",
      " [  13    6   14  863]]\n",
      " accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "savemodel = 'finalized_model.sav'\n",
    "rf = pickle.load(open(savemodel, 'rb'))\n",
    "\n",
    "# Load testdata\n",
    "savetestdata = 'testdata.sav'\n",
    "testdata = pickle.load(open(savetestdata, 'rb'))\n",
    "X_test = testdata[0]\n",
    "y_test = testdata[1]\n",
    "\n",
    "# Evaluate the new classifier on test data\n",
    "ypred_test_rf, rf_test_accuracy, rf_test_confmat = eval_classifier(rf, X_test, y_test)\n",
    "report_eval('Random Forest New - TEST', rf_test_accuracy, rf_test_confmat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPY Widget\n",
    "#### How it works:\n",
    "- This file opens .jpg files from the directory \"demo_images\". The images are loaded and reshaped if needed. It is possible to put some images in yourself in order to see how the classifier performs. The classifier works with square images, so make sure to make square images.\n",
    "- Once the images are loaded, the classifier predicts the output and the results are printed in a ipywidget. In the widget there are some sliders which can be manipulated:\n",
    " - n - which is the n'th image in the directory\n",
    " - showgridpoints - show/hide the grid sampe points that are used to find the tip and the end of the fork, the purple left triangles are the points that have a probability higher that 90% of being the class \"tip\", the purple right triangles are the points that have a probability higher that 90% of being the class \"end\". The blue crosses are the mean value of all the left- and right triangle.\n",
    " - showpoint - show/hide the extra sample points which are casted around the previously found blue crosses. The cyan stars indicate the mean of the points that have a probability higher that 90% of being the class \"tip\" or \"end\".\n",
    " - showfilter - show/hide the filter that is used in preprocessing of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 10 images in target directory\n",
      "loaded 10 images\n",
      "...all 10 images have been transformed to B&W and filtered...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32983252eee54815a52ff02b77e03b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='n', max=9), IntSlider(value=0, description='showgridpoin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_pred_, Ibwf_ = image_predictior('images/demo_images',rf, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
